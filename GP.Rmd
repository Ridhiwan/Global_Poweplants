---
title: <center> Global Poweplants Fuel Analysis </center>
author: <center> Ridhiwan Mseya </center>
date: <center> 02/12/2022 </center>
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
 
<br>
 
<br>
  
## 1. INTRODUCTION

&nbsp;&nbsp;&nbsp;&nbsp; The world is in need of reliable and affordable green power sector. The shift towards cleaner energy that is environmentally sustainable forces us to rethink and reinvent better sources of energy to power our societies. To do this we first have to take a zoomed out look at the sources of energy around the world, the level of dependency towards those sources and their effects on the environment. Governments and companies will now make decisions that will depend on the power sector and its transformation towards green energy. For example, if a carbon tax is to be passed to certain sources of energy then the price of electricity will increase and affect prices of products and thus the economy.

&nbsp;&nbsp;&nbsp;&nbsp; In this analysis we will use "The Global Plant Database" which is an open source and open access dataset of grid scale(1MW and greater) electricity generating facilities in the world. This dataset consists of the country, name of the power plant, the capacity in megawatts, the latitude and longitude of the plant, the primary fuel for power generation, year of commission, the generated electricity in gigawatt hours and more. We will incorporate the available data into a predictive model that guesses the primary fuel of a plant anywhere in the world based on its location, capacity and other parameters available in the data. Below is a quick view of our database:

```{r, message=FALSE, warning=FALSE}
library(tibble)
library(readr)
library(tidyverse)

gppb <- read.csv("global_power_plant_database.csv", header = TRUE)

gppb1 <- tibble(gppb)

head(gppb1)

```


&nbsp;&nbsp;&nbsp;&nbsp; The ability to predict the type of fuel that could be used in a given location is essential in planning the type of emissions and waste that could be associated with electricity generation in that area. A pre-plan on how to mitigate the effects of the emissions and waste products will help in making the production process greener. New environment friendly technology can be developed years ahead of electricity production and specific to that location.

<br>
 
<br>
  
## 2. METHODS


&nbsp;&nbsp;&nbsp;&nbsp; In this section we will visualize some aspects of our data, do some correlations and clustering tests, clean the data, alter its form if necessary and make a model using different methods like kmeans and random forest.


### Clean the Data

&nbsp;&nbsp;&nbsp;&nbsp; Before we perform any analysis we will first remove all the data that may cause inconsistency in our analysis.Some of the information in the columns in our data are not necessary for our analysis i.e. name of the country, name of power plant, other fuels, year of data generation.

```{r, message=FALSE, warning=FALSE}
# Choose columns wit the most essential data

gppb2 <- gppb1 %>% select(gppd_idnr,capacity_mw,latitude,longitude,primary_fuel,
                          generation_gwh_2013,generation_gwh_2014,generation_gwh_2015,generation_gwh_2016,generation_gwh_2017,estimated_generation_gwh)

head(gppb2)

# Assume the energy production was zero in the years that it was not recorded
gppb2[is.na(gppb2)] <- 0
head(gppb2)

```




### Correlation

&nbsp;&nbsp;&nbsp;&nbsp; Correlation analysis of the variables in our database.

```{r, message=FALSE, warning=FALSE}
library(corrplot)

# factorize the character columns
gppb3 <- gppb2
gppb3$gppd_idnr <- as.numeric(as.factor(gppb3$gppd_idnr))
gppb3$primary_fuel <- as.numeric(as.factor(gppb3$primary_fuel))
head(gppb3)

# Plot the correlation figure
corrplot(cor(gppb3),method = "color",type = "upper")

```

There is very little correlation between primary fuel and the latitude and longitudes of the powerplant. This is good because our model based on these parameters will have less bias. We also observe very high correlation between the generated energy from 2013 to 2017 because the capacity of the powerplant increases slightly in the scale of gigawatts throughout the years. We will remake these 5 energy columns into one column by finding the average over the 5 years.

```{r, message=FALSE, warning=FALSE}
# Average the energy generation columns into one

gppb3$avg_gwh <- rowMeans(gppb3[,c(6:11)], na.rm = TRUE)

gppb4 <- gppb3 %>% select(-c(generation_gwh_2013,generation_gwh_2014,generation_gwh_2015,generation_gwh_2016,generation_gwh_2017,estimated_generation_gwh))
head(gppb4)


```

At this point any power plant that displays an average energy generation of zero should be removed because it has missing data for all the given years and will affect our model negatively. Then we will re-plot our correlation plot.

```{r, message=FALSE, warning=FALSE}
# remove rows with zero average energy generation
gppb5 <- gppb4[gppb4$avg_gwh != 0,]

#plot new correlation plot
corrplot(cor(gppb5),method = "color",type = "upper")
```

In this plot there is stronger correlation between average energy generation and the capacity of the power plant which makes sense.There is also an interesting negative correlation between the capacity of the power plant and the primary fuel. Same goes for primary fuel and the average energy generated. These could mean that our generation power and capacity strongly depend on the primary fuel, which seems obvious.


### Visualization

&nbsp;&nbsp;&nbsp;&nbsp; We will investigate some of the features observable in our data. Let us start with the mean energy that each primary fuel has produced all over the world.

```{r, message=FALSE, warning=FALSE}
library(ggplot2)

# Add a column with string characters for primary fuel
gppb4$primary_fuel_chr <- add_column(gppb2$primary_fuel)

# Create a plot
gppb4 %>% select(avg_gwh,primary_fuel_chr) %>% group_by(primary_fuel_chr) %>%
  summarise(mean_gwh = mean(avg_gwh)) %>% ggplot(aes(primary_fuel_chr,mean_gwh)) + geom_bar(stat = "identity", color = "blue", fill = "blue") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

```

We can observe that on average the most power producing plants are those with the primary fuel of nuclear, Coal, petcoke and gas. Nuclear energy has very high output per powerplant compared to any of the other fuels. Now let us view the most total energy produced by each fuel around the world.

```{r, message=FALSE, warning=FALSE}

# Create a plot
gppb4 %>% select(avg_gwh,primary_fuel_chr) %>% group_by(primary_fuel_chr) %>%
  summarise(total_gwh = sum(avg_gwh)) %>% ggplot(aes(primary_fuel_chr,total_gwh))+  geom_bar(stat = "identity", color = "blue", fill = "blue") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

```

In terms of the total energy produced Coal leads the way followed by Gas. Hydro and Nuclear almost have the same contribution. The rest of the renewable energy sources have little contribution into powering the world.


 In order to do this we will first need to   do some stratification of our data.The strata of the rounded off longitude and latitudes will cover a greater region than the specific position provided by the original database.

```{r, message=FALSE, warning=FALSE}

# Round off the latitude and longitudes into integers for stratification
gppb6 <- gppb5
gppb6$longitude <- round(gppb6$longitude)
gppb6$latitude <- round(gppb6$latitude)
head(gppb6)
```

### CLustering

#### K-Means

The first model will be unsupervised in which we will do clustering using the k-means algorithm. We will get rid of the id,longitude and latitude columns of the powerplants and rescale all the values of the remaining columns.

```{r, message=FALSE, warning=FALSE}
#edit and rescale the dataset
gppb7 <- gppb6 %>% mutate(capacity_mw = scale(capacity_mw),
                          longitude = scale(longitude),
                          primary_fuel = scale(primary_fuel),
                          avg_gwh = scale(avg_gwh)) %>% select(-c(gppd_idnr,latitude))

head(gppb7)
```

Let us animate the clustering using longitude and primary fuel to observe the process of clustering and the type of separation that is possible. We will choose the number of clusters to be less than half the number of different fuel types for illustration purposes.

```{r, message=FALSE, warning=FALSE}
# Make the first animation of longitude and primary fuel
set.seed(2233)
library(animation)
ani.options(nmax = 3)
kmeans.ani(gppb7[2:3],6, pch = 1:6, col = 1:6)
```

As it has been shown above from the many steps that the k-means algorithm takes to move centers and find clusters, our data can be clustered but knowing the number of clusters to start with is not easy. We will use a strategy that will give us a graph that will help us choose the most optimal number of clusters to use for our analysis. The elbow method is a good method for large datasets as it looks at the variance as a function of the number of clusters.

```{r, message=FALSE, warning=FALSE}
# Analyse the best number of clusters
k.max <- 15
data <- gppb7
wss <- sapply(1:k.max, 
              function(k){kmeans(data, k, nstart=50,iter.max = 15 )$tot.withinss})
wss
```

```{r, message=FALSE, warning=FALSE}
# plot total within sum of squares for each cluster count
tab <- cbind.data.frame(1:k.max,wss)
colnames(tab) <- c("k","ws")
tab %>% ggplot(aes(k, ws)) + geom_line() +geom_point()
```

6 looks like the optimal number of clusters for our data as the change seems to be minimal as we keep adding more clusters after that point.

```{r, message=FALSE, warning=FALSE}
# Redo kmeans with optimal k
set.seed(2233)
clst <- kmeans(gppb7,6)
clst$size
centr <- clst$centers
centr
```

From the cluster size we can see that there is some sort of homogeinity in the clusters which might be good. Let us dig deeper into the centers and see how the features stack up into the clusters.

```{r, message=FALSE, warning=FALSE}
# create dataset for reshape
cluster <- c(1: 6)
centered <- data.frame(cluster, centr)

# Reshape the data
reshaped <- gather(centered, feature, value, capacity_mw: avg_gwh)
head(reshaped,10)
```

Let us now create the heatmap to visualize the intesity of features in clusters.

```{r, message=FALSE, warning=FALSE}
library(RColorBrewer)
# Create the palette
heat.palette <-colorRampPalette(rev(brewer.pal(10, 'RdYlGn')),space='Lab')

#make the plot of the heatmap
reshaped %>% ggplot(aes(x = feature, y = cluster, fill = value)) +
    scale_y_continuous(breaks = seq(1, 6, by = 1)) +
    geom_tile() +
    coord_equal() +
    scale_fill_gradientn(colours = heat.palette(90)) +
    theme_classic() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

The sixth cluster which was the smallest cluster by size, has a high intensity of two features that are related to energy production while the rest of the clusters have an almost homogeneous distribution of the features in each cluster. The latitudes with unsupervised learning can be grouped into 6 clusters.


### Models

The models that we will develop have a goal of predicting the most probable primary fuel to be used at a given location once we know the latitude and longitude of that area. We will use the latitude as the predictor in our models.

#### Random Forest

Random forest is a supervised learning approach that uses decisions of the many to predict the most probable result for a given model. For our dataset we will train a model and choose the best parameters for prediction accordingly.

Let us start by training our model on small different datasets and then combine their results. This will need us to split our data into test and train sets since I have a lot of observables around 29,000 thus I can split my data by 80/20 or 90/10 or somewhere in between as the variance of my parameters will be okay with either of the two. For the purpose of easier computation, I will use the 80/20 split.

```{r, message=FALSE, warning=FALSE}
library(caret)
library(e1071)

# Split the data into train and test sets
# Here we use the version of our datasets that contains-
# all important columns with original values
set.seed(2233)
test_ind <- createDataPartition(as.factor(gppb5$primary_fuel),p=0.2, list = FALSE, times = 1)

test_data <- gppb5[test_ind,]
train_data <- gppb5[-test_ind,]

head(test_data)

```

```{r, message=FALSE, warning=FALSE}
library(randomForest)
library(doParallel)

set.seed(1558)

# set up parallelism
cores <- 3
clt <- makePSOCKcluster(cores)
registerDoParallel(clt)

# choose parameters for training
trControl <- trainControl(method = "cv",
    number = 7,
    search = "grid")

# tune the grid 
tunegrid = expand.grid(.mtry=c(1:20))

# train the model
rf_model <- train(as.factor(primary_fuel)~., sampsize=7000,train_data, method = "parRF", ntree=300, tuneGrid = tunegrid, trControl = trControl)

#the loop
loop_rf <- foreach(ntree=rep(300,cores),
                   .combine = combine,
                   .multicombine = TRUE,
                   .packages = "randomForest") %dopar% 
  randomForest(as.factor(primary_fuel)~., sampsize =7000,train_data, ntree=ntree, tuneGrid = tunegrid)

stopCluster(clt)
print(rf_model)
```

From the above analysis the best parameter for mtry is 3. We will repeat our model with this parameter and try to fine tune the sample size and the number of times we do cross validations.

```{r, message=FALSE, warning=FALSE}
library(randomForest)
library(doParallel)

set.seed(1558)

# set up parallelism
cores <- 3
clt <- makePSOCKcluster(cores)
registerDoParallel(clt)

# set mtry
tunegrid = expand.grid(.mtry = 3)

# choose parameters for training
trControl <- trainControl(method = "cv",
    number = 10,
    search = "grid")

# train the model
rf_model1 <- train(as.factor(primary_fuel)~., sampsize=10000,train_data, method = "parRF", ntree=500, tuneGrid = tunegrid, trControl = trControl)

#the loop
loop_rf <- foreach(ntree=rep(500,cores),
                   .combine = combine,
                   .multicombine = TRUE,
                   .packages = "randomForest") %dopar% 
  randomForest(as.factor(primary_fuel)~., sampsize =10000,train_data, ntree=ntree, tuneGrid = tunegrid)

stopCluster(clt)
print(rf_model1)
```

Our accuracy with our second model is 0.85 which is not bad considering our sample size of only 10000 out of the 23000 available samples. We will stop our tuning at this point and check for the prediction power of the model next.

```{r, message=FALSE, warning=FALSE}
set.seed(1558)
# make the prediction
pred <- predict(rf_model1,test_data)

# Check the accuracy of the prediction
acc <- confusionMatrix(pred, as.factor(test_data$primary_fuel))
acc$overall
```

Our prediction parameters give as an accuracy of 0.85 which is good for a first time model to which we have not done extensive parameter optimization due to time and computational cost limitations.



####  Support Vector Machines

&nbsp;&nbsp;&nbsp;&nbsp; Random Forest is known for over fitting data in its models. We will use Support Vector Machines(SVMs) to try and get a prediction that is more discriminating in its classification. Better classes will lead to better prediction.

We have already split our data and trained it in the previous part. This time we will only train our data and see how well the new model performs.

```{r, message=FALSE, warning=FALSE}
set.seed(1558)
#train the model
svm_model <- svm(as.factor(primary_fuel)~., train_data, type = "C-classification", kernel = "linear")
print(svm_model)
```

Now let us check the prediction accuracy for our initial svm analysis.

```{r, message=FALSE, warning=FALSE}
set.seed(1558)
# make the prediction
pred <- predict(svm_model,test_data)

# Check the accuracy of the prediction
acc <- mean(test_data$primary_fuel==pred)
acc
```
Let us increase the cost of our classifier to see if the accuracy will increase from 0.35 which is very bad.

```{r, message=FALSE, warning=FALSE}
set.seed(1558)
#train the model
svm_model1 <- svm(as.factor(primary_fuel)~., train_data, cost = 20, type = "C-classification", kernel = "linear")
```

```{r, message=FALSE, warning=FALSE}
set.seed(1558)
# make the prediction
pred <- predict(svm_model1,test_data)

# Check the accuracy of the prediction
acc <- mean(test_data$primary_fuel==pred)
acc
```

Increasing the cost from the default 0f 1 to 20 makes a very small improvement that is negligible. This time we will adjust the type of kernel used and see how better our model gets.

```{r, message=FALSE, warning=FALSE}
set.seed(1558)
#train the model
svm_model2 <- svm(as.factor(primary_fuel)~., train_data, type = "C-classification", kernel = "radial")
```

```{r, message=FALSE, warning=FALSE}
set.seed(1558)
# make the prediction
pred <- predict(svm_model2,test_data)

# Check the accuracy of the prediction
acc <- mean(test_data$primary_fuel==pred)
acc
```

With radial kernel our prediction becomes a bit better than random chance. We will again try increasing the cost and see if it will have a major improvement or minor one.

```{r, message=FALSE, warning=FALSE}
set.seed(1558)
#train the model
svm_model3 <- svm(as.factor(primary_fuel)~., train_data,cost = 200, type = "C-classification", kernel = "radial")
```

```{r, message=FALSE, warning=FALSE}
set.seed(1558)
# make the prediction
pred <- predict(svm_model3,test_data)

# Check the accuracy of the prediction
acc <- mean(test_data$primary_fuel==pred)
acc
```
In order to achieve at least 70% accuracy our cost of misclassification has to be very high and the decision surface is far from smooth.


### Results


